{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Youtube_comment-Model-Sentiment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yopiazani02/Deep-Learning/blob/master/Youtube_comment_Model_Sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffL9ozgzyiP1",
        "colab_type": "text"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBiCiN-WyVdJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import unicodedata\n",
        "import re\n",
        "# from pandas import DataFrame\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ7IeKd0W5ir",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6QmAJg6ydk3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#datasets komentar youtube pada video Update Corona 22 Maret: 514 Kasus, 48 Meninggal, 29 Sembuh\n",
        "dataset = pd.read_csv('/content/drive/My Drive/ytb_comment.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MBEeI0mygMX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data\n",
        "dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "useTcKUazZM4",
        "colab_type": "text"
      },
      "source": [
        "# Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd6jUW-33GJS",
        "colab_type": "code",
        "outputId": "fbfc7ec2-205d-4a91-8a85-81d71aa4db16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "pip install contractions"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/85/41/c3dfd5feb91a8d587ed1a59f553f07c05f95ad4e5d00ab78702fbf8fe48a/contractions-0.0.24-py2.py3-none-any.whl\n",
            "Collecting textsearch\n",
            "  Downloading https://files.pythonhosted.org/packages/42/a8/03407021f9555043de5492a2bd7a35c56cc03c2510092b5ec018cae1bbf1/textsearch-0.0.17-py2.py3-none-any.whl\n",
            "Collecting Unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 3.2MB/s \n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/9f/f0d8e8850e12829eea2e778f1c90e3c53a9a799b7f412082a5d21cd19ae1/pyahocorasick-1.4.0.tar.gz (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 12.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.0-cp36-cp36m-linux_x86_64.whl size=81692 sha256=5e43bc0d84fc9863bd8b258aa69a628111419a8c38694f2935975eccd329f483\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/90/61/87a55f5b459792fbb2b7ba6b31721b06ff5cf6bde541b40994\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: Unidecode, pyahocorasick, textsearch, contractions\n",
            "Successfully installed Unidecode-1.1.1 contractions-0.0.24 pyahocorasick-1.4.0 textsearch-0.0.17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0hgARaD3NUW",
        "colab_type": "code",
        "outputId": "d4142ad6-1fa4-4b85-e2d7-f619eee20824",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "pip install sastrawi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sastrawi\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/4b/bab676953da3103003730b8fcdfadbdd20f333d4add10af949dd5c51e6ed/Sastrawi-1.0.1-py2.py3-none-any.whl (209kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: sastrawi\n",
            "Successfully installed sastrawi-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wc4Ct87A3syV",
        "colab_type": "code",
        "outputId": "4ba1fe62-4761-4c8d-c19f-11e700403227",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V42sXgSDzbwJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import contractions\n",
        "import inflect\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvrFEZmNuQjK",
        "colab_type": "text"
      },
      "source": [
        "Masukkan stopwords_id.txt kedalam folder\n",
        "\n",
        "`/root/nltk_data/corpora/stopwords`\n",
        "\n",
        "kemudian rename menjadi stopwords_id (hilangkan extention txt)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1W_QSKpz0gc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def removeStopword(str):\n",
        "    stop_words = set(stopwords.words('stopwords_id'))\n",
        "    word_tokens = word_tokenize(str)\n",
        "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
        "    return ' '.join(filtered_sentence)\n",
        "\n",
        "def stemming(str):\n",
        "    factory = StemmerFactory()\n",
        "    stemmer = factory.create_stemmer()\n",
        "    return stemmer.stem(str)\n",
        "\n",
        "def cleaning(str):\n",
        "    #remove non-ascii\n",
        "    str = unicodedata.normalize('NFKD', str).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    #remove URLs\n",
        "    str = re.sub(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))', '', str)\n",
        "    #remove punctuations\n",
        "    str = re.sub(r'[^\\w]|_',' ',str)\n",
        "    #remove digit from string\n",
        "    str = re.sub(\"\\S*\\d\\S*\", \" \", str).strip()\n",
        "    #remove digit or numbers\n",
        "    str = re.sub(r\"\\b\\d+\\b\", \" \", str)\n",
        "    #to lowercase\n",
        "    str = str.lower()\n",
        "    #Remove additional white spaces\n",
        "    str = re.sub('[\\s]+', ' ', str)\n",
        "    # print(str)   \n",
        "    return str\n",
        "\n",
        "def slangWord(str):\n",
        "  # str = str.lower()\n",
        "  # file = '/slang.dic'\n",
        "  file = '/content/drive/My Drive/Colab Notebooks/slang.dic'\n",
        "  fix2 = {}\n",
        "  with open(file) as f:\n",
        "    for line in f:\n",
        "       (key, val) = line.split(':')\n",
        "       fix2[key] = val.replace('\\n',' ')\n",
        "  tweet_t = nltk.word_tokenize(str)\n",
        "  clean = \" \"\n",
        "  for i in tweet_t:\n",
        "    for slang, formal in fix2.items():\n",
        "        if i == slang:\n",
        "            i = formal\n",
        "    if i.isalnum():\n",
        "        clean = clean+i+' '\n",
        "        # clean = clean+i\n",
        "        # clean = ' '.join(clean)\n",
        "    else:\n",
        "        clean = clean[:-1]+' '+i+' '\n",
        "        # clean = clean[:-1]+i\n",
        "        # clean = ' '.join(clean)\n",
        "  str = clean\n",
        "  # print(str)\n",
        "  return str\n",
        "\n",
        "def preprocessing(str):\n",
        "    # print(str)\n",
        "    \n",
        "    str = cleaning(str)\n",
        "    str = slangWord(str)        \n",
        "    str = removeStopword(str)\n",
        "    str = stemming(str)\n",
        "    \n",
        "    return str"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZtX0SebsGFm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# comment = dataset['Comment']\n",
        "comment = dataset['Comment'][:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvBTCMncsiyb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "comment"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ub5tXfUIsk3b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_clean = []\n",
        "data_text = dataset['Comment']\n",
        "for item in data_text:\n",
        "  tmp = preprocessing(item)\n",
        "  data_clean.append(tmp)\n",
        "\n",
        "dataset['Comment'] = data_clean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGoU6NRNRGgY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRGt1RKRMiFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Buat fungsi sentiment analysis menggunakan TextBlob\n",
        "from textblob import TextBlob\n",
        "import re\n",
        "import numpy as np\n",
        "import requests\n",
        "import time\n",
        "\n",
        "def analize_sentiment(comment):\n",
        "  # print(comment)\n",
        "  try:\n",
        "    analysis = TextBlob(str(comment))\n",
        "    time.sleep(0.2)\n",
        "    toEN = analysis.translate(to='en')\n",
        "    time.sleep(0.2)\n",
        "    if toEN.sentiment.polarity > 0:\n",
        "      return('positif')\n",
        "    elif toEN.sentiment.polarity == 0:\n",
        "      return('netral')\n",
        "    else:\n",
        "      return('negatif')\n",
        "  except:\n",
        "    return ('netral')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbYf6eYDMlol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tambahkan kolom label ke dalam dataframe:\n",
        "dataset['label'] = np.array([analize_sentiment(comment) for comment in dataset['Comment'] ])\n",
        "\n",
        "# Tampilkan data:\n",
        "display(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8_0BUf9Rgat",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos_tweets = [ tweet for index, tweet in enumerate(dataset['Comment']) if dataset['label'][index] == 'positif']\n",
        "neu_tweets = [ tweet for index, tweet in enumerate(dataset['Comment']) if dataset['label'][index] == 'netral']\n",
        "neg_tweets = [ tweet for index, tweet in enumerate(dataset['Comment']) if dataset['label'][index] == 'negatif']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_uAnNzHRjBk",
        "colab_type": "code",
        "outputId": "fef98dfa-e542-4b96-f1d0-8a673961ad44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "total_pos = len(pos_tweets)*100/len(dataset['Comment'])\n",
        "total_net = len(neu_tweets)*100/len(dataset['Comment'])\n",
        "total_neg = len(neg_tweets)*100/len(dataset['Comment'])\n",
        "\n",
        "print(\"Total Positif: \", total_pos)\n",
        "print(\"Total Netral: \", total_net)\n",
        "print(\"Total Negatif: \", total_neg)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Positif:  0.0\n",
            "Total Netral:  100.0\n",
            "Total Negatif:  0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2hhhrb0qTI0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#menghapus kolom username pada datasets\n",
        "dataset = dataset.drop(columns=\"Username\")\n",
        "dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pz17HVeWqUYR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#export data to CSV\n",
        "dataset.to_csv('/content/drive/My Drive/clean_data1.csv', index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}